{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: beautifulsoup4 in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: lxml in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: html5lib==1.0b8 in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: six in ./.local/lib/python3.5/site-packages (from html5lib==1.0b8)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade beautifulsoup4 lxml html5lib==1.0b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def funtion(url,cookies):\n",
    "    resp = requests.get(\n",
    "        url, \n",
    "        cookies=cookies\n",
    "    )\n",
    "\n",
    "    soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "    gossip_list = soup.find('div', {'class': 'r-list-container action-bar-margin bbs-screen'})\n",
    "\n",
    "    a_list = gossip_list.find_all('a')\n",
    "    #print(type(a_list))\n",
    "\n",
    "    bbs_url_list = [link.get('href') for link in a_list]\n",
    "\n",
    "    bbs_text_list = [link.text for link in a_list]\n",
    "\n",
    "    author_list = gossip_list.find_all('div', {'class': 'author'})\n",
    "    author_list = [link.text for link in author_list]\n",
    "    \n",
    "    re_dic = {\n",
    "        'bbs_url_list': bbs_url_list,\n",
    "        'bbs_text_list': bbs_text_list,\n",
    "        'author_list': author_list\n",
    "    }\n",
    "    \n",
    "    return re_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ptt.cc/bbs/Gossiping/index29647.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29646.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29645.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29644.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29643.html\n",
      "23\n",
      "23\n",
      "24\n",
      " \n",
      "20\n",
      "20\n",
      "20\n",
      " \n",
      "20\n",
      "20\n",
      "20\n",
      " \n",
      "19\n",
      "19\n",
      "20\n",
      " \n",
      "19\n",
      "19\n",
      "20\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# up_url_list = ['1','2','3','4','5']\n",
    "up_url_list = []\n",
    "\n",
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "\n",
    "for i in range(0, 5):\n",
    "    resp = requests.get(\n",
    "        url, \n",
    "        cookies={'over18': '1'}\n",
    "    )\n",
    "    soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "    page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "    up_list = page_list.find_all('a')\n",
    "#     up_url_list[i] = 'https://www.ptt.cc' + up_list[1].get('href')\n",
    "    up_url_list.append('https://www.ptt.cc' + up_list[1].get('href'))\n",
    "    url = up_url_list[i]\n",
    "    \n",
    "page_info = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(up_url_list[i])\n",
    "    page_info.append(funtion(up_url_list[i],{'over18': '1'}))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(len(page_info[i]['bbs_url_list']))\n",
    "    print(len(page_info[i]['bbs_text_list']))\n",
    "    print(len(page_info[i]['author_list']))\n",
    "    print(' ')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PttCrawler:\n",
    "    def __init__(self, board, write= False ):\n",
    "        self.ptt_url = 'https://www.ptt.cc'\n",
    "        self.board = board\n",
    "        self.now_page = board\n",
    "        self.session = requests.Session()\n",
    "        self.session.cookies.update({\n",
    "            'over18':'1'\n",
    "        })\n",
    "        self.write=write\n",
    "        \n",
    "        \n",
    "    def page_info(self,url):\n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        gossip_list = soup.find('div', {'class': 'r-list-container action-bar-margin bbs-screen'})\n",
    "\n",
    "        a_list = gossip_list.find_all('div',{'r-ent'})\n",
    "        \n",
    "        re_dic = []\n",
    "        \n",
    "        for link in a_list:\n",
    "            bbs_a = link.find('a')\n",
    "            \n",
    "            author = link.find('div', {'class': 'author'})\n",
    "            date = link.find('div', {'class': 'date'})\n",
    "            \n",
    "            bbs_url = 'https://www.ptt.cc' + bbs_a.get('href')\n",
    "    \n",
    "            resp = self.session.get(\n",
    "                bbs_url\n",
    "            )\n",
    "            soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "            \n",
    "            bbs_content = soup.find('div',{'id': 'main-content'})\n",
    "            \n",
    "            content = ''\n",
    "            s = list(bbs_content.strings)\n",
    "            for i in range(8,len(s)-1):\n",
    "                content = content + s[i]\n",
    "            \n",
    "            re_dic.append({\n",
    "                'bbs_url': bbs_url,\n",
    "                'bbs_text': bbs_a.text,\n",
    "                'author': author.text,\n",
    "                'date': date.text,\n",
    "                'content': content\n",
    "            })\n",
    "        \n",
    "        return re_dic\n",
    "    \n",
    "    def previous_page_url(self,url):\n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "        up_list = page_list.find_all('a')\n",
    "        if not up_list[1].get('href'):\n",
    "            re_str = 'this is first page!'\n",
    "        else:\n",
    "            re_str = 'https://www.ptt.cc' + up_list[1].get('href')\n",
    "            self.now_page = re_str\n",
    "        return self.now_page\n",
    "    \n",
    "    def next_page_url(self,url):\n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "        up_list = page_list.find_all('a')\n",
    "        if not up_list[2].get('href'):\n",
    "            re_str = 'this is last page!'\n",
    "        else:\n",
    "            re_str = 'https://www.ptt.cc' + up_list[2].get('href')\n",
    "            self.now_page = re_str\n",
    "        return self.now_page\n",
    "    \n",
    "    def to_page(self,page):\n",
    "        re_str = self.board\n",
    "        for i in range(0, page):\n",
    "            re_str = self.previous_page(re_str)\n",
    "        self.now_page = re_str\n",
    "        return self.now_page\n",
    "    \n",
    "    @property\n",
    "    def show_now_page(self):\n",
    "        return self.now_page\n",
    "    \n",
    "    @property\n",
    "    def next_page(self):\n",
    "        self.next_page_url(self.now_page)\n",
    "        return self.now_page\n",
    "    \n",
    "    @property\n",
    "    def previous_page(self):\n",
    "        self.previous_page_url(self.now_page)\n",
    "        return self.now_page\n",
    "    \n",
    "    @property\n",
    "    def return_to_board(self):\n",
    "        self.now_page = self.board\n",
    "        return self.now_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ptt.cc/bbs/Gossiping/index29652.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29651.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29652.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29652.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index.html\n",
      "13\n",
      "[問卦] 延續剛剛酒駕事件\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1513851901.A.CC7.html\n",
      "chamge7788\n",
      "\n",
      "※ 八卦板務請到 GossipPicket 檢舉板詢問\n",
      "※ a.張貼問卦請注意，充實文章內容、是否有專板，本板並非萬能問板。\n",
      "※ b.一天只能張貼 \"兩則\" 問卦，自刪及被刪也算兩篇之內，\n",
      "※   超貼者將被水桶，請注意!\n",
      "※ c.本看板嚴格禁止政治問卦，發文問卦前請先仔細閱讀相關板規。\n",
      "※ d.未滿30繁體中文字視同鬧板，請注意！\n",
      "※ (↑看完提醒請刪除ctrl + y)\n",
      "各位鄉民大家好 我剛剛看了很多人噓文 首先酒駕不對 但是當天回收很多非常多\n",
      "\n",
      "我爸只是幫忙載而已 當天我可以說\n",
      "\n",
      "我爸並沒有一喝酒 就開車\n",
      "\n",
      "而是 過了11小時才去幫忙\n",
      "\n",
      "當天是週六 車被寄放在拖到場所\n",
      "\n",
      "過年我們家都不好過 做業務的 只因為幫忙載回收 沒有一喝酒就開車 因為是過年\n",
      "\n",
      "所以無法當天領車 而且 我們家在那時候\n",
      "\n",
      "只有爸爸有駕照 現在沒有了 由我哥來開車 我哥駕照考了 11次才過\n",
      "\n",
      "很難 尤其是路考 連我方向感很好的人\n",
      "\n",
      "都68分沒過 回到正題 而且我是左撇子喔\n",
      "\n",
      "左撇子沒有人路考一次過的\n",
      "\n",
      "我可以說 回收過多導致我們家 經濟受影響 但是酒駕就是不對 但是以經過了11小時才開\n",
      "車 而且是幫忙載回收喔 卻要掉扣\n",
      "\n",
      "駕照 因為是業務員 不能開車了 害我哥只能去上班當作業原 無法考公務員 因為當時家\n",
      "中沒有經濟支柱\n",
      "\n",
      "總結 如果當下喝酒開車 不該原諒是垃圾\n",
      "\n",
      "撞傷人 判死或鞭刑\n",
      "\n",
      "如果有過10小時以上 但是酒測值0.24以下 一公里內 且做善事著 可只罰錢 不用掉扣駕\n",
      "照 這對一個家庭影響很大 而且駕照很難考 尤其是路考\n",
      "\n",
      "有沒有惡酒駕根善酒駕的卦\n",
      "\n",
      "善酒駕 當下喝酒不開車 過10小時才開\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 1.169.61.190\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1513851901.A.CC7.html\n",
      "12/21\n"
     ]
    }
   ],
   "source": [
    "test = PttCrawler('https://www.ptt.cc/bbs/Gossiping/index.html')\n",
    "print(test.previous_page_url('https://www.ptt.cc/bbs/Gossiping/index.html'))\n",
    "print(test.previous_page)\n",
    "print(test.next_page)\n",
    "print(test.now_page)\n",
    "print(test.return_to_board)\n",
    "\n",
    "re_dic = test.page_info(test.now_page)\n",
    "print(len(re_dic))\n",
    "print(re_dic[0]['bbs_text'])\n",
    "print(re_dic[0]['bbs_url'])\n",
    "print(re_dic[0]['author'])\n",
    "print(re_dic[0]['content'])\n",
    "print(re_dic[0]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[問卦] 忠孝東路為什麼要走九遍\n",
      "/bbs/Gossiping/M.1513851291.A.FD7.html\n",
      "alan4ni\n",
      "\n",
      "大家好\n",
      "\n",
      "我是從我家走到巷口就快累死的艾倫\n",
      "\n",
      "聽說啊動力火車有一首歌叫做忠孝東路走九遍\n",
      "\n",
      "但忠孝東路真的是超級長的起點從監察院到南港都還是忠孝東路\n",
      "\n",
      "一趟走完起碼要3 4個小時吧\n",
      "\n",
      "為什麼還要走九遍勒 幹嘛不挑南京東路 或 民權東路勒\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 117.19.73.223\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1513851291.A.FD7.html\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "\n",
    "resp = requests.get(\n",
    "    url, \n",
    "    cookies={'over18': '1'}\n",
    ")\n",
    "soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "gossip_list = soup.find('div', {'class': 'r-list-container action-bar-margin bbs-screen'})\n",
    "\n",
    "r_list = gossip_list.find_all('div',{'class': 'r-ent'})\n",
    "\n",
    "re_dic = []\n",
    "\n",
    "for link in r_list:\n",
    "    bbs_a = link.find('a')\n",
    "    author = link.find('div', {'class': 'author'})\n",
    "    \n",
    "    bbs_url = 'https://www.ptt.cc' + bbs_a.get('href')\n",
    "    \n",
    "    resp = requests.get(\n",
    "        bbs_url,\n",
    "        cookies={'over18': '1'}\n",
    "    )\n",
    "    soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "    bbs_content = soup.find('div',{'id': 'main-content'})\n",
    "    \n",
    "    content = ''\n",
    "    s = list(bbs_content.strings)\n",
    "    for i in range(8,len(s)-1):\n",
    "        content = content + s[i]\n",
    "    \n",
    "    re_dic.append({\n",
    "        'bbs_url': bbs_a.get('href'),\n",
    "        'bbs_text': bbs_a.text,\n",
    "        'author': author.text,\n",
    "        'content': content\n",
    "    })\n",
    "    \n",
    "print(len(re_dic))\n",
    "print(re_dic[0]['bbs_text'])\n",
    "print(re_dic[0]['bbs_url'])\n",
    "print(re_dic[0]['author'])\n",
    "print(re_dic[0]['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
