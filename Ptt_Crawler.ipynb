{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: beautifulsoup4 in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: lxml in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: html5lib==1.0b8 in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: six in ./.local/lib/python3.5/site-packages (from html5lib==1.0b8)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade beautifulsoup4 lxml html5lib==1.0b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def funtion(url,cookies):\n",
    "    resp = requests.get(\n",
    "        url, \n",
    "        cookies=cookies\n",
    "    )\n",
    "\n",
    "    soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "    gossip_list = soup.find('div', {'class': 'r-list-container action-bar-margin bbs-screen'})\n",
    "\n",
    "    a_list = gossip_list.find_all('a')\n",
    "    #print(type(a_list))\n",
    "\n",
    "    bbs_url_list = [link.get('href') for link in a_list]\n",
    "\n",
    "    bbs_text_list = [link.text for link in a_list]\n",
    "\n",
    "    author_list = gossip_list.find_all('div', {'class': 'author'})\n",
    "    author_list = [link.text for link in author_list]\n",
    "    \n",
    "    re_dic = {\n",
    "        'bbs_url_list': bbs_url_list,\n",
    "        'bbs_text_list': bbs_text_list,\n",
    "        'author_list': author_list\n",
    "    }\n",
    "    \n",
    "    return re_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ptt.cc/bbs/Gossiping/index29647.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29646.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29645.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29644.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29643.html\n",
      "23\n",
      "23\n",
      "24\n",
      " \n",
      "20\n",
      "20\n",
      "20\n",
      " \n",
      "20\n",
      "20\n",
      "20\n",
      " \n",
      "19\n",
      "19\n",
      "20\n",
      " \n",
      "19\n",
      "19\n",
      "20\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# up_url_list = ['1','2','3','4','5']\n",
    "up_url_list = []\n",
    "\n",
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "\n",
    "for i in range(0, 5):\n",
    "    resp = requests.get(\n",
    "        url, \n",
    "        cookies={'over18': '1'}\n",
    "    )\n",
    "    soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "    page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "    up_list = page_list.find_all('a')\n",
    "#     up_url_list[i] = 'https://www.ptt.cc' + up_list[1].get('href')\n",
    "    up_url_list.append('https://www.ptt.cc' + up_list[1].get('href'))\n",
    "    url = up_url_list[i]\n",
    "    \n",
    "page_info = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(up_url_list[i])\n",
    "    page_info.append(funtion(up_url_list[i],{'over18': '1'}))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(len(page_info[i]['bbs_url_list']))\n",
    "    print(len(page_info[i]['bbs_text_list']))\n",
    "    print(len(page_info[i]['author_list']))\n",
    "    print(' ')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PttCrawler:\n",
    "    def __init__(self, board, write= False ):\n",
    "        self.ptt_url = 'https://www.ptt.cc'\n",
    "        self.board = board\n",
    "        self.now_page = board\n",
    "        self.session = requests.Session()\n",
    "        self.session.cookies.update({\n",
    "            'over18':'1'\n",
    "        })\n",
    "        self.write=write\n",
    "        \n",
    "        \n",
    "    def page_info(self,url):\n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        gossip_list = soup.find('div', {'class': 'r-list-container action-bar-margin bbs-screen'})\n",
    "\n",
    "        a_list = gossip_list.find_all('div',{'r-ent'})\n",
    "        \n",
    "        re_dic = []\n",
    "        \n",
    "        for link in a_list:\n",
    "            bbs_a = link.find('a')\n",
    "            \n",
    "            author = link.find('div', {'class': 'author'})\n",
    "            date = link.find('div', {'class': 'date'})\n",
    "            \n",
    "            bbs_url = 'https://www.ptt.cc' + bbs_a.get('href')\n",
    "    \n",
    "            resp = requests.get(\n",
    "                bbs_url,\n",
    "                cookies={'over18': '1'}\n",
    "            )\n",
    "            soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "            \n",
    "            bbs_content = soup.find('div',{'id': 'main-content'})\n",
    "            \n",
    "            content = ''\n",
    "            s = list(bbs_content.strings)\n",
    "            for i in range(8,len(s)-1):\n",
    "                content = content + s[i]\n",
    "            \n",
    "            re_dic.append({\n",
    "                'bbs_url': bbs_url,\n",
    "                'bbs_text': bbs_a.text,\n",
    "                'author': author.text,\n",
    "                'date': date.text,\n",
    "                'content': content\n",
    "            })\n",
    "        \n",
    "        return re_dic\n",
    "    \n",
    "    def previous_page_url(self,url):\n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "        up_list = page_list.find_all('a')\n",
    "        if not up_list[1].get('href'):\n",
    "            re_str = 'this is first page!'\n",
    "        else:\n",
    "            re_str = 'https://www.ptt.cc' + up_list[1].get('href')\n",
    "            self.now_page = re_str\n",
    "        return self.now_page\n",
    "    \n",
    "    def next_page_url(self,url):\n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "        up_list = page_list.find_all('a')\n",
    "        if not up_list[2].get('href'):\n",
    "            re_str = 'this is last page!'\n",
    "        else:\n",
    "            re_str = 'https://www.ptt.cc' + up_list[2].get('href')\n",
    "            self.now_page = re_str\n",
    "        return self.now_page\n",
    "    \n",
    "    def to_page(self,page):\n",
    "        re_str = self.board\n",
    "        for i in range(0, page):\n",
    "            re_str = self.previous_page(re_str)\n",
    "        self.now_page = re_str\n",
    "        return self.now_page\n",
    "    \n",
    "    @property\n",
    "    def show_now_page(self):\n",
    "        return self.now_page\n",
    "    \n",
    "    @property\n",
    "    def next_page(self):\n",
    "        self.next_page_url(self.now_page)\n",
    "        return self.now_page\n",
    "    \n",
    "    @property\n",
    "    def previous_page(self):\n",
    "        self.previous_page_url(self.now_page)\n",
    "        return self.now_page\n",
    "    \n",
    "    @property\n",
    "    def return_to_board(self):\n",
    "        self.now_page = self.board\n",
    "        return self.now_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ptt.cc/bbs/Gossiping/index29651.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29650.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29651.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29651.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index.html\n",
      "18\n",
      "[問卦] 忠孝東路為什麼要走九遍\n",
      "https://www.ptt.cc/bbs/Gossiping/M.1513851291.A.FD7.html\n",
      "alan4ni\n",
      "\n",
      "大家好\n",
      "\n",
      "我是從我家走到巷口就快累死的艾倫\n",
      "\n",
      "聽說啊動力火車有一首歌叫做忠孝東路走九遍\n",
      "\n",
      "但忠孝東路真的是超級長的起點從監察院到南港都還是忠孝東路\n",
      "\n",
      "一趟走完起碼要3 4個小時吧\n",
      "\n",
      "為什麼還要走九遍勒 幹嘛不挑南京東路 或 民權東路勒\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 117.19.73.223\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1513851291.A.FD7.html\n",
      "12/21\n"
     ]
    }
   ],
   "source": [
    "test = PttCrawler('https://www.ptt.cc/bbs/Gossiping/index.html')\n",
    "print(test.previous_page_url('https://www.ptt.cc/bbs/Gossiping/index.html'))\n",
    "print(test.previous_page)\n",
    "print(test.next_page)\n",
    "print(test.now_page)\n",
    "print(test.return_to_board)\n",
    "\n",
    "re_dic = test.page_info(test.now_page)\n",
    "print(len(re_dic))\n",
    "print(re_dic[0]['bbs_text'])\n",
    "print(re_dic[0]['bbs_url'])\n",
    "print(re_dic[0]['author'])\n",
    "print(re_dic[0]['content'])\n",
    "print(re_dic[0]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[問卦] 忠孝東路為什麼要走九遍\n",
      "/bbs/Gossiping/M.1513851291.A.FD7.html\n",
      "alan4ni\n",
      "\n",
      "大家好\n",
      "\n",
      "我是從我家走到巷口就快累死的艾倫\n",
      "\n",
      "聽說啊動力火車有一首歌叫做忠孝東路走九遍\n",
      "\n",
      "但忠孝東路真的是超級長的起點從監察院到南港都還是忠孝東路\n",
      "\n",
      "一趟走完起碼要3 4個小時吧\n",
      "\n",
      "為什麼還要走九遍勒 幹嘛不挑南京東路 或 民權東路勒\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 117.19.73.223\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1513851291.A.FD7.html\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "\n",
    "resp = requests.get(\n",
    "    url, \n",
    "    cookies={'over18': '1'}\n",
    ")\n",
    "soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "gossip_list = soup.find('div', {'class': 'r-list-container action-bar-margin bbs-screen'})\n",
    "\n",
    "r_list = gossip_list.find_all('div',{'class': 'r-ent'})\n",
    "\n",
    "re_dic = []\n",
    "\n",
    "for link in r_list:\n",
    "    bbs_a = link.find('a')\n",
    "    author = link.find('div', {'class': 'author'})\n",
    "    \n",
    "    bbs_url = 'https://www.ptt.cc' + bbs_a.get('href')\n",
    "    \n",
    "    resp = requests.get(\n",
    "        bbs_url,\n",
    "        cookies={'over18': '1'}\n",
    "    )\n",
    "    soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "    bbs_content = soup.find('div',{'id': 'main-content'})\n",
    "    \n",
    "    content = ''\n",
    "    s = list(bbs_content.strings)\n",
    "    for i in range(8,len(s)-1):\n",
    "        content = content + s[i]\n",
    "    \n",
    "    re_dic.append({\n",
    "        'bbs_url': bbs_a.get('href'),\n",
    "        'bbs_text': bbs_a.text,\n",
    "        'author': author.text,\n",
    "        'content': content\n",
    "    })\n",
    "    \n",
    "print(len(re_dic))\n",
    "print(re_dic[0]['bbs_text'])\n",
    "print(re_dic[0]['bbs_url'])\n",
    "print(re_dic[0]['author'])\n",
    "print(re_dic[0]['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
