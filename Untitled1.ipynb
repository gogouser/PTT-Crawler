{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: beautifulsoup4 in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: lxml in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: html5lib==1.0b8 in ./.local/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: six in ./.local/lib/python3.5/site-packages (from html5lib==1.0b8)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade beautifulsoup4 lxml html5lib==1.0b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def funtion(url,cookies):\n",
    "    resp = requests.get(\n",
    "        url, \n",
    "        cookies=cookies\n",
    "    )\n",
    "\n",
    "    soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "    gossip_list = soup.find('div', {'class': 'r-list-container action-bar-margin bbs-screen'})\n",
    "\n",
    "    a_list = gossip_list.find_all('a')\n",
    "    #print(type(a_list))\n",
    "\n",
    "    bbs_url_list = [link.get('href') for link in a_list]\n",
    "\n",
    "    bbs_text_list = [link.text for link in a_list]\n",
    "\n",
    "    author_list = gossip_list.find_all('div', {'class': 'author'})\n",
    "    author_list = [link.text for link in author_list]\n",
    "    \n",
    "    re_dic = {\n",
    "        'bbs_url_list': bbs_url_list,\n",
    "        'bbs_text_list': bbs_text_list,\n",
    "        'author_list': author_list\n",
    "    }\n",
    "    \n",
    "    return re_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ptt.cc/bbs/Gossiping/index29631.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29630.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29629.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29628.html\n",
      "https://www.ptt.cc/bbs/Gossiping/index29627.html\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "19\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# up_url_list = ['1','2','3','4','5']\n",
    "up_url_list = []\n",
    "\n",
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "\n",
    "for i in range(0, 5):\n",
    "    resp = requests.get(\n",
    "        url, \n",
    "        cookies={'over18': '1'}\n",
    "    )\n",
    "    soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "    page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "    up_list = page_list.find_all('a')\n",
    "#     up_url_list[i] = 'https://www.ptt.cc' + up_list[1].get('href')\n",
    "    up_url_list.append('https://www.ptt.cc' + up_list[1].get('href'))\n",
    "    url = up_url_list[i]\n",
    "    \n",
    "page_info = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(up_url_list[i])\n",
    "    page_info.append(funtion(up_url_list[i],{'over18': '1'}))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(len(page_info[i]['bbs_url_list']))\n",
    "    print(len(page_info[i]['bbs_text_list']))\n",
    "    print(len(page_info[i]['author_list']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PttCrawler:\n",
    "    def __init__(self, board, write= False ):\n",
    "        self.ptt_url = 'https://www.ptt.cc'\n",
    "        self.board = board+''\n",
    "        self.last_page = board+''\n",
    "        self.session = requests.Session()\n",
    "        self.session.cookies.update({\n",
    "            'over18':'1'\n",
    "        })\n",
    "        self.write=write\n",
    "        \n",
    "    def page_info(self,url):\n",
    "        if not url:\n",
    "            url = self.last_page\n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        gossip_list = soup.find('div', {'class': 'r-list-container action-bar-margin bbs-screen'})\n",
    "\n",
    "        a_list = gossip_list.find_all('a')\n",
    "        #print(type(a_list))\n",
    "\n",
    "        bbs_url_list = [link.get('href') for link in a_list]\n",
    "\n",
    "        bbs_text_list = [link.text for link in a_list]\n",
    "\n",
    "        author_list = gossip_list.find_all('div', {'class': 'author'})\n",
    "        author_list = [link.text for link in author_list]\n",
    "\n",
    "        re_dic = {\n",
    "            'bbs_url_list': bbs_url_list,\n",
    "            'bbs_text_list': bbs_text_list,\n",
    "            'author_list': author_list\n",
    "        }\n",
    "\n",
    "        return re_dic\n",
    "    \n",
    "    def previous_page(self,url):\n",
    "        if not url:\n",
    "            url = self.last_page\n",
    "        \n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "        up_list = page_list.find_all('a')\n",
    "        if not up_list[1].get('href'):\n",
    "            re_str = 'this is first page!'\n",
    "        else:\n",
    "            re_str = 'https://www.ptt.cc' + up_list[1].get('href')\n",
    "            self.now_page = re_str\n",
    "        return re_str\n",
    "    \n",
    "    def next_page(self,url):\n",
    "        if not url:\n",
    "            url = self.last_page\n",
    "        \n",
    "        resp = self.session.get(\n",
    "            url\n",
    "        )\n",
    "        soup = BeautifulSoup(resp.text.encode('utf-8'), 'lxml')\n",
    "\n",
    "        page_list = soup.find('div', {'class': 'btn-group btn-group-paging'})\n",
    "        up_list = page_list.find_all('a')\n",
    "        if not up_list[2].get('href'):\n",
    "            re_str = 'this is last page!'\n",
    "        else:\n",
    "            re_str = 'https://www.ptt.cc' + up_list[2].get('href')\n",
    "            self.now_page = re_str\n",
    "        return re_str\n",
    "    \n",
    "    def to_page(self,page):\n",
    "        re_str = self.board\n",
    "        for i in range(0, page):\n",
    "            re_str = self.previous_page(re_str)\n",
    "        self.last_page = re_str\n",
    "        return re_str\n",
    "    \n",
    "    def show_last_page(self):\n",
    "        return self.last_page\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PttCrawler.show_last_page of <__main__.PttCrawler object at 0x7f0350dfbbe0>>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = PttCrawler('https://www.ptt.cc/bbs/Gossiping/index.html')\n",
    "test.previous_page('https://www.ptt.cc/bbs/Gossiping/index.html')\n",
    "test.show_last_page"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
